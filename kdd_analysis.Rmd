---
title: "KDD Decision Tree Analysis"
author: "Daniel Davis"
date: "September 27, 2020"
output: html_document
---

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 200)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Description

In this project two training sets are selected from the 10% KDD Cup 1999 Dataset, located at http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html. Decision trees are used for classification of attack types.

### Data Retrieval and Preprocessing

The dataset was downloaded and extracted to a csv file, stored at Z:\kddcup.csv
The features for this dataset are stored in a .names file, stored at Z:\kddcup.names

The KDD 10% Training Dataset, containing 494,020 records is used for this project The dataset and headers are in two separate files, which will be read in and preprocessed below.

``` {r include = FALSE, message = FALSE}
library(caret)
library(party)
library(partykit)
library('FSelector')
```

``` {r include = TRUE}
kdd <- read.csv("Z:\\kddcup.csv", header = FALSE, stringsAsFactors = FALSE)
headers <- read.delim("Z:\\kddcup.names", header = FALSE, stringsAsFactors = FALSE, sep = ":")

#Get the data classification from the first entry
classificationTypes <- headers[1,1]
classificationTypes <- unlist(strsplit(headers[1,1],","))

#Remove the trailing period from the last classificationType entry.
x <- classificationTypes[length(classificationTypes)]
x <- substr(x, 1, nchar(x)-1)
classificationTypes[length(classificationTypes)] <- x

headers <- headers[[1]]
headers <- headers[-1]
headers[length(headers) + 1] <- "classification"

colnames(kdd) <- headers

#Write the dataset with headers to a new file
#write.csv(kdd, "Z:\\kddcupWithHeaders.csv")
```

### Data Exploration
``` {r include = TRUE}
length(unique(kdd$classification))

unique(kdd$classification)
```

There are 23 unique attack types in the KDD 10% dataset, belonging to four different general classification types in addition to normal traffic (CUP-99 Task Description). These five values will be used for classification.

``` {r include = TRUE}
#Set a vector for each classification type

dos <- c("back.", "land.", "neptune.", "pod.", "smurf.","teardrop.")
probe <- c("ipsweep.", "nmap.", "portsweep.", "satan.")
u2r <- c("buffer_overflow.", "loadmodule.", "perl.", "rootkit.")
r2l <- c("ftp_write.", "guess_passwd.", "imap.", "multihop.", "phf.", "spy.", "warezmaster.", "warezclient.")
```

We see that there is a large disparity in the type of attacks represented in the dataset, which may be rectified with undersampling and oversampling. A new column is created with the classification type mapped to each attack.

``` {r include = FALSE}
#Copy the attack types to a separate vector and assign the attack types
#in kdd$classification to the approriate category.
kddAttackTypes <- kdd$classification

kdd$classification[which(kdd$classification %in% dos)] = "dos"
kdd$classification[which(kdd$classification %in% probe)] = "probe"
kdd$classification[which(kdd$classification %in% r2l)] = "r2l"
kdd$classification[which(kdd$classification %in% u2r)] = "u2r"
kdd$classification[which(kdd$classification %in% "normal.")] = "normal"
```

``` {r include = FALSE}
#All columns were read as strings for easy replacement of classification types.
#To prevent errors creating decision trees
#the character columns will be converted to factor columns before subsets of the kdd dataset are created.

changeCols <- which((sapply(kdd, class) == "character") == TRUE)

kdd[,changeCols] <- lapply(kdd[,changeCols], as.factor)
```

``` {r echo = FALSE}
#Print the representation of each classification type
kddFrequencies <- as.data.frame(table(kdd$classification))
kddFrequencies$percent <- sapply(kddFrequencies$Freq, function(x) {x/length(kdd$classification)})
kddFrequencies
```

### Sample Creation

Below, a testing and a validation dataset are created using random sampling. A sample size of 4900 will be used for each, 1% of the total dataset. A separate testing sample will be used in order to evaluate both decision trees against a sample with the same distribution as the population dataset, without relying on the validation set.

``` {r include = TRUE}
set.seed(341)
sampleSize <- 9800
sampleVector <- sample(c(1:length(kdd$classification)), sampleSize)

kddCut <- kdd[sampleVector,]
#Remove the test and validation records from the kdd dataset
kdd <- kdd[-sampleVector,]

sampleSize <- sampleSize/2
sampleCut <- sample(c(1:length(sampleVector)), sampleSize)

kddTest <- kddCut[sampleCut,]

kddValidation <- kddCut[-sampleCut,] 

kddcut = NULL
```

The distribution of the test set is displayed:
``` {r echo = FALSE}
#Print the class frequencies for the test set
kddSampleFrequencies <- as.data.frame(table(kddTest$classification))
kddSampleFrequencies$percent <- sapply(kddSampleFrequencies$Freq, function(x) {x/sampleSize})
kddSampleFrequencies
```

The distribution of the validation set is displayed:
``` {r echo = FALSE}
#Print the class frequencies for the validation set
kddSampleFrequencies <- as.data.frame(table(kddValidation$classification))
kddSampleFrequencies$percent <- sapply(kddSampleFrequencies$Freq, function(x) {x/sampleSize})
kddSampleFrequencies
```

The test and validation sets closely matches the population distribution. At least one of each classification type is included in each set.


Our first training sample will also be selected using random sampling:
``` {r echo = FALSE}
set.seed(283)
sampleSize = 4900
sampleVector <- sample(c(1:length(kdd$classification)), sampleSize)
kddTrain1 <- kdd[sampleVector,]

#Print the class frequencies for the training set
kddSampleFrequencies <- as.data.frame(table(kddTrain1$classification))
kddSampleFrequencies$percent <- sapply(kddSampleFrequencies$Freq, function(x) {x/sampleSize})
kddSampleFrequencies
```
This sample also contains a similar distribution to the population; however it is important to note that only a single U2R record is included.


The second training sample will be oversampled on Probe, R2L and U2R records, and undersampled on DOS and Normal records. Stratified sampling procedure is sourced from Malato, 2019.
``` {r echo = FALSE}
#Get the frequencies for each classification type.
#Stratified sampling from Malato, 2019.
kddClassFreq <- as.data.frame(table(kdd$classification))
set.seed(288)
sampleSize = 4900

#Assign the frequencies to a vector matching the kdd dataframe
kddFreq <- c(1,rep(length(kdd$classification)))
kddFreq[which(kdd$classification %in% "dos")] = 0.05
kddFreq[which(kdd$classification %in% "probe")] = 0.1
kddFreq[which(kdd$classification %in% "r2l")] = 2
kddFreq[which(kdd$classification %in% "u2r")] = 3
kddFreq[which(kdd$classification %in% "normal")] = 0.05

#Create a sample using the frequency vector.
sampleVector <- sample(c(1:length(kdd$classification)), sampleSize, prob = kddFreq)
kddTrain2 <- kdd[sampleVector,]

#Print the class frequencies for the training set
kddSampleFrequencies <- as.data.frame(table(kddTrain2$classification))
kddSampleFrequencies$percent <- sapply(kddSampleFrequencies$Freq, function(x) {x/sampleSize})
kddSampleFrequencies
```

Using a probability vector the likelihood of selection of each class has been highly modified, but with a relatively small impact on the classification distribution in the sample data set. The frequency of R2L and U2R records have both been increased by one order of magnitude with relatively little change to the other classes.

### Decision Trees

Decision trees for each sample will be created using the R Party package's implementation of the Conditional Inference Tree algorithm. To ensure that the class distribution discrepancy in this dataset is accounted for, the Kappa statistic will be used as the training criteria.

#### Random Distribution Sample Decision Tree

Using the FSelector library, the information gain for each classification will be calculated for the first training sample.

``` {r include = TRUE}
set.seed(134)
sampleImportance <- gain.ratio(classification~., kddTrain1)

sampleImportance$names <- rownames(sampleImportance)
sampleImportance <- sampleImportance[order(-sampleImportance$attr_importance),]
rownames(sampleImportance) <- NULL
head(sampleImportance,21)
```
The first 20 attributes ranked by information gain are within the same order of magnitude, showing a range between 0.7 and 0.1, after which the attribute information gain drops off to no information gain. It is important to note that one or more of these attributes may be significant for attack types not well-represented in this sample.

``` {r include = TRUE}
#Create a cTree classifier
control <- trainControl(method = "cv")
set.seed(742)
classifier1 <- train(classification ~ ., data = kddTrain1,method='ctree', metric ="Kappa", trControl = control)

classifier1

train1Classes <- predict(classifier1, newdata = kddTest[,-42])

confusionMatrix(train1Classes, kddTest$classification)
```

![Conditional Inference Tree from Random Distribution Training Sample](Z:\\classes\\IS678\\Assignment 1\\classifier1Tree.png)



#### Adjusted Distribution Sample Decision Tree

``` {r include = TRUE}
set.seed(884)
sampleImportance2 <- gain.ratio(classification~., kddTrain2)

sampleImportance2$names <- rownames(sampleImportance2)
sampleImportance2 <- sampleImportance2[order(-sampleImportance2$attr_importance),]
rownames(sampleImportance2) <- NULL
head(sampleImportance2, 34)
```
For the adjusted distribution sample we see a much flatter distribution in information gain.


``` {r include = TRUE}
#Create a cTree classifier
control <- trainControl(method = "cv")
set.seed(742)
classifier2 <- train(classification ~ ., data = kddTrain2,method='ctree', metric ="Kappa", trControl = control)

classifier2

train2Classes <- predict(classifier2, newdata = kddTest[,-42])

confusionMatrix(train2Classes, kddTest$classification)
```

![Conditional Inference Tree from Adjusted Distribution Training Sample](Z:\\classifier2Tree.png)



### Comparison

The decision tree created from the adjusted distribution sample is much more complex, 
We see that the decision tree created from the random distribution training sample did perform better than the adjusted distribution training sample. This is not necessarily surprising, as the validation set had the same random distribution. The random distribution decision tree did fail to accurately classify any R2L or U2R attacks, which the adjusted distribution decision tree performed better on.

It is important to note that only a few attributes are shared between the two decision trees. Better performance may be acheived by cutting down the number of variables. Below, the attributes most relevant for separating U2R attack types and R2L attack types from other classification types are explored.


#### What are the most important attributes for U2R and R2L attacks?

In order to answer this question, we can re-run the information gain algorithm on a reclassified sample dataset.
Shown below is the procedure applied to copies of the random sampled distribution training dataset. This is likely to give more appropriate results for the population dataset considering there similar distributions. The procedure performed on the adjusted distribution dataset, not shown, did show similar results however.

``` {r echo = FALSE}
#Convert the classes of the first training sample
notU2R <- c("dos", "probe", "r2l", "normal")
kddTrain1Cut <- kddTrain1
kddTrain1Cut$classification <- sapply(kddTrain1Cut$classification, as.character)
kddTrain1Cut$classification[which(kddTrain1Cut$classification %in% notU2R)] = "notU2R"
kddTrain1Cut$classification <- as.factor(kddTrain1Cut$classification)

set.seed(334)
sampleImportanceU2R <- gain.ratio(classification~., kddTrain1Cut)

sampleImportanceU2R$names <- rownames(sampleImportanceU2R)
sampleImportanceU2R <- sampleImportanceU2R[order(-sampleImportanceU2R$attr_importance),]
rownames(sampleImportanceU2R) <- NULL
head(sampleImportanceU2R)
```

``` {r echo = FALSE}
#Convert the classes of the first training sample
notR2L <- c("dos", "probe", "u2r", "normal")
kddTrain1Cut <- kddTrain1
kddTrain1Cut$classification <- sapply(kddTrain1Cut$classification, as.character)
kddTrain1Cut$classification[which(kddTrain1Cut$classification %in% notR2L)] = "notR2L"
kddTrain1Cut$classification <- as.factor(kddTrain1Cut$classification)

set.seed(444)
sampleImportanceR2L <- gain.ratio(classification~., kddTrain1Cut)

sampleImportanceR2L$names <- rownames(sampleImportanceR2L)
sampleImportanceR2L <- sampleImportanceR2L[order(-sampleImportanceR2L$attr_importance),]
rownames(sampleImportanceR2L) <- NULL
head(sampleImportanceR2L)
```

We see that service, protocol type, and flag are the only attributes found by the information gain algorithm that have an impact on distinguising U2L attacks from normal traffic or other attack types. The attributes shown to have the greatest impact for R2L attacks were already represented in the decision trees seen above. Below, the attributes used in the decision trees above, along with Service, Protocol Type, and Flag attributes are subsetted from the kdd dataset. All other attributes besides classification are removed from the training samples.

### Decision Trees using targeted attributes

In order to leverage this knowledge, two new decision trees will be created, using only the attributes selected by the first set of decision trees, along with the service, protocol type, and flag attributes shown to be of importance for R2L and U2R attack types.

``` {r include = TRUE}
attributeList <- c("logged_in", "dst_host_srv_count", "src_bytes", "root_shell", "servicesmtp", "servicehttp", "count", "dst_hos_same_srv_rate", "srv_count", "dst_host_diff_srv_rate", "num_compromised", "protocol_typeudp", "dst_host_count", "wrong_fragment", "service", "protocol_type", "flag", "classification")
kdd <- kdd[,which(colnames(kdd) %in% attributeList)]
```

#### Create a CTree classifier from the random sample distribution with targeted attributes

``` {r include = TRUE}
#Create a cTree classifier
control <- trainControl(method = "cv")
set.seed(742)

kddTrain1 <- kddTrain1[,which(colnames(kddTrain1) %in% attributeList)]
classifier1Targeted <- train(classification ~ ., data = kddTrain1,method='ctree', metric ="Kappa", trControl = control)

classifier1Targeted


train1Classes <- predict(classifier1Targeted, newdata = kddTest[,-42])

confusionMatrix(train1Classes, kddTest$classification)
```

![Conditional Inference Tree from Random Distribution Training Sample with Selected Attributes ](Z:\\classifier1TreeTargeted.png)



#### Create a CTree classifier from the adjusted sample distribution with targeted attributes
``` {r include = TRUE}
#Create a cTree classifier
control <- trainControl(method = "cv")
set.seed(742)

kddTrain2 <- kddTrain2[,which(colnames(kddTrain2) %in% attributeList)]
classifier2Targeted <- train(classification ~ ., data = kddTrain2,method='ctree', metric ="Kappa", trControl = control)

classifier2Targeted

train2Classes <- predict(classifier2Targeted, newdata = kddTest[,-42])

confusionMatrix(train2Classes, kddTest$classification)
```

![Conditional Inference Tree from Adjusted Distribution Training Sample with Selected Attributes ](Z:\\classes\\IS678\\Assignment 1\\classifier2TreeTargeted.png)



With targeted attributes we see that the random distribution decision tree and the adjusted distribution tree had very similar performance. The random distribution tree actually performed better against R2L and U2L attacks than the adjusted distribution tree, which had eight times as many records misidentified as R2L attacks than there actually were. This is likely the result of the adjusted distribution itself.

In terms of attack vs. normal traffic classification in prediction of the test set:
The random distribution tree misidentified 17 attacks as normal traffic, and 17 normal traffic records were misidentified as attacks.
The adjusted distribution tree misidentified 11 attacks as normal traffic, and 51 normal traffic records were misidentified as attacks.

Below, the two trees are used to predict the classification of the validation set:

Random Distribution Against Validation Set:
``` {r include = TRUE}
validation1Classes <- predict(classifier1Targeted, newdata = kddValidation[,-42])

confusionMatrix(validation1Classes, kddValidation$classification)
```

Adjusted Distribution Against Validation Set:
``` {r include = TRUE}
validation2Classes <- predict(classifier2Targeted, newdata = kddValidation[,-42])

confusionMatrix(validation2Classes, kddValidation$classification)
```
The two decision trees showed similar performance against the validation set as the test set.

### Discussion

In this analysis we saw that adjusting class distribution had little affect on conditional inference tree performance. This is likely only the case as all classes were represented to some degree in all sets. It is also true that the attack classes were imbalanced to such a degree that with the first random distribution tree, the algorithm may have discounted classifying R2L and U2R attacks. We did see improvement in both the random distribution and adjusted distribution trained decision trees when attributes were selected specifically rather than using all available. More domain knowledge would likely lead to better selection of attributes.

### References

CUP-99 Task Description. (n.d.). Retrieved September 17, 2020, from http://kdd.ics.uci.edu/databases/kddcup99/task.html

Malato, G. (2019, May 07). Stratified sampling and how to perform it in R. Retrieved September 17, 2020, from https://towardsdatascience.com/stratified-sampling-and-how-to-perform-it-in-r-8b753efde1ef